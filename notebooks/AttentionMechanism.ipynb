{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mwerR0hePgXn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "ACNEnxuSg_zZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len , d_model = inputs.shape"
      ],
      "metadata": {
        "id": "KFVu7bJLvNsf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = 2\n",
        "\n",
        "W_q = torch.nn.Parameter(torch.randn(d_model, d_k))\n",
        "W_k = torch.nn.Parameter(torch.randn(d_model, d_k))\n",
        "W_v = torch.nn.Parameter(torch.randn(d_model, d_k))"
      ],
      "metadata": {
        "id": "pcKALb_tuxL7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(X, W_q, W_k, W_v):\n",
        "  seq_len = X.shape[0]\n",
        "  d_k = W_q.shape[1]\n",
        "\n",
        "  Q = X @ W_q\n",
        "  K = X @ W_k\n",
        "  V = X @ W_v\n",
        "\n",
        "  scores = (Q @ K.T) / (d_k ** 0.5)\n",
        "\n",
        "  mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "  scores = scores.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "  attn_weights = F.softmax(scores, dim =-1)\n",
        "  output = attn_weights @V\n",
        "\n",
        "  return output, attn_weights"
      ],
      "metadata": {
        "id": "6ufXS50phEVA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weighs = self_attention(inputs, W_q, W_k, W_v)\n",
        "\n",
        "print(output)\n",
        "print(attn_weighs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8Vwuj6uG8a",
        "outputId": "a9191b72-9d09-4011-e606-018d06358a55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4509, 0.6372],\n",
            "        [0.3008, 1.3833],\n",
            "        [0.2660, 1.6186],\n",
            "        [0.1883, 1.4945],\n",
            "        [0.3701, 1.4193],\n",
            "        [0.2607, 1.3759]], grad_fn=<MmBackward0>)\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4744, 0.5256, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3029, 0.3433, 0.3539, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2553, 0.2492, 0.2529, 0.2426, 0.0000, 0.0000],\n",
            "        [0.1134, 0.1870, 0.1915, 0.2028, 0.3053, 0.0000],\n",
            "        [0.2000, 0.1578, 0.1601, 0.1462, 0.2088, 0.1271]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  implementing trainable weights with backprop"
      ],
      "metadata": {
        "id": "Hq7qGCawvum3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "94QGKHLowkgp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, d_k):\n",
        "    super().__init__()\n",
        "    self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
        "    self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
        "    self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "  def forward(self, X):\n",
        "    seq_len = X.shape[0]\n",
        "\n",
        "    Q = self.W_q(X)\n",
        "    K = self.W_k(X)\n",
        "    V = self.W_v(X)\n",
        "\n",
        "    scores = (Q @ K.T) / (d_k ** 0.5)\n",
        "\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    scores = scores.masked_fill(mask ==1 , float('-inf'))\n",
        "\n",
        "    attn_weights = F.softmax(scores, dim =-1)\n",
        "    output = attn_weights @ V\n",
        "\n",
        "    return output, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "98AcCeRy3G3W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 3\n",
        "d_k = 2\n",
        "attn = SelfAttention(d_model, d_k)\n",
        "\n",
        "target = inputs.clone().detach()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mS5biyFIrHVH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(attn.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "aEwzKcsEsIpp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(10):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  output, _ = attn(inputs)\n",
        "  loss = F.mse_loss(output, target[:, :2])\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IIC1KCf4qcAs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi head attenation"
      ],
      "metadata": {
        "id": "p2aJqsgu1bnu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_model % num_heads == 0\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_head = d_model // num_heads\n",
        "    # Fix: The qkv_proj should project to 3 * d_model to account for Q, K, V concatenated\n",
        "    self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "    self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "  def forward(self, X):\n",
        "    seq_len = X.shape[0]\n",
        "\n",
        "    qkv = self.qkv_proj(X) # Now qkv will have shape (seq_len, 3 * d_model)\n",
        "    Q, K, V = qkv.chunk(3, dim =-1) # Each of Q, K, V will have shape (seq_len, d_model)\n",
        "\n",
        "    # Reshape Q, K, V to (seq_len, num_heads, d_head)\n",
        "    # Since d_model = num_heads * d_head, reshaping (seq_len, d_model) to (seq_len, num_heads, d_head) is valid\n",
        "    Q = Q.view(seq_len, self.num_heads, self.d_head)\n",
        "    K = K.view(seq_len, self.num_heads, self.d_head)\n",
        "    V = V.view(seq_len, self.num_heads, self.d_head)\n",
        "\n",
        "    Q = Q.transpose(0,1)\n",
        "    K = K.transpose(0,1)\n",
        "    V = V.transpose(0,1)\n",
        "\n",
        "    scores = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    scores = scores.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "    attn = F.softmax(scores, dim =-1)\n",
        "    out = attn @ V\n",
        "\n",
        "    out = out.transpose(0,1).contiguous()\n",
        "    out = out.view(seq_len, self.d_model)\n",
        "\n",
        "    return self.out_proj(out)"
      ],
      "metadata": {
        "id": "wR1NCyxU3mJf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input embeddings\n",
        "X = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],\n",
        "     [0.55, 0.87, 0.66],\n",
        "     [0.57, 0.85, 0.64],\n",
        "     [0.22, 0.58, 0.33],\n",
        "     [0.77, 0.25, 0.10],\n",
        "     [0.05, 0.80, 0.55]],\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "# Model\n",
        "d_model = 3\n",
        "num_heads = 1  # must divide d_model\n",
        "mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "# Forward pass\n",
        "output = mhsa(X)\n",
        "print(output.shape)  # (6, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgczGUZe9lDC",
        "outputId": "5d6cf5e1-c690-4722-c48d-a985d3b5ea2a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqzH3i269qTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}